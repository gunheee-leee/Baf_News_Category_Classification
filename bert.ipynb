{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106df5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6944d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bba6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.read_csv(\"C:/Users/82102/Desktop/Baf_News_Category_Classification/summarized2.csv\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['Unnamed: 0','text','length'],inplace=True,axis=1)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.dropna()\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5184fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['text_length'] = all_data['summarized_text'].apply(len)\n",
    "\n",
    "# text_length 열에서 가장 큰 값을 찾아 출력\n",
    "max_length = all_data['text_length'].max()\n",
    "\n",
    "print(\"가장 긴 문자열 길이:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b25cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(111)\n",
    "np.random.seed(111)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "NUM_EPOCHS=8\n",
    "VALID_SPLIT=0.2\n",
    "MAX_LEN=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# LabelEncoder 생성\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# \"category\" 열의 라벨을 인코딩\n",
    "all_data['label'] = label_encoder.fit_transform(all_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 각 클래스별로 데이터 분류\n",
    "class_data = {}\n",
    "for target_class in all_data['target'].unique():\n",
    "    class_data[target_class] = all_data[all_data['target'] == target_class]\n",
    "\n",
    "# 각 클래스별로 80%는 훈련 데이터, 20%는 테스트 데이터로 분할\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for target_class, data in class_data.items():\n",
    "    train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 하나의 데이터프레임으로 병합\n",
    "train_data = pd.concat(train_list)\n",
    "test_data = pd.concat(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e17f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.sample(frac=1)\n",
    "train_data.reset_index(drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data.sample(frac=1)\n",
    "test_data.reset_index(drop=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt',do_lower_case=False)\n",
    "\n",
    "def bert_tokenizer(sentence, MAX_LEN):\n",
    "\n",
    "    encoded_dict=tokenizer.encode_plus(\n",
    "      text = sentence,\n",
    "      add_special_tokens=True,\n",
    "      max_length=MAX_LEN,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True\n",
    "  )\n",
    "\n",
    "    input_id=encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    token_type_id=encoded_dict['token_type_ids']\n",
    "\n",
    "      return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "token_type_ids=[]\n",
    "train_data_labels=[]\n",
    "\n",
    "for train_sentence, train_label in tqdm(zip(train_data['summarized_text'], train_data['label']), total=len(train_data)):\n",
    "      try:\n",
    "    input_id, attention_mask, token_type_id = bert_tokenizer(train_sentence, MAX_LEN)\n",
    "\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "    train_data_labels.append(train_label)\n",
    "      except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "train_news_input_ids=np.array(input_ids, dtype=int)\n",
    "train_news_attention_masks=np.array(attention_masks, dtype=int)\n",
    "train_news_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "train_news_inputs=(train_news_input_ids, train_news_attention_masks, train_news_token_type_ids)\n",
    "train_data_labels=np.asarray(train_data_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id=train_news_input_ids[19]\n",
    "attention_mask=train_news_attention_masks[1]\n",
    "token_type_id=train_news_token_type_ids[1]\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(train_news_input_ids[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "      def __init__(self, model_name, dir_path, num_class):\n",
    "    super(TFBertClassifier, self).__init__()\n",
    "\n",
    "    self.bert=TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "    self.dropout=tf.keras.layers.Dropout(0.2) # 데이터 양이 적어서 0.5로 지정 #self.bert.config_hidden_dropout_prob\n",
    "    self.classifier=tf.keras.layers.Dense(num_class,\n",
    "                                          kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                          name='classifier')\n",
    "\n",
    "      def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "    outputs=self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    pooled_output=outputs[1]\n",
    "    pooled_output=self.dropout(pooled_output, training=training)\n",
    "    logits=self.classifier(pooled_output)\n",
    "\n",
    "    return logits\n",
    "\n",
    "cls_model=TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                           dir_path='bert_ckpt',\n",
    "                           num_class=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4952e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(2e-6)\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric=tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=cls_model.fit(train_news_inputs, train_data_labels,\n",
    "                      epochs=20, batch_size=BATCH_SIZE, validation_split=VALID_SPLIT)\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0622359",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'], '')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "token_type_ids=[]\n",
    "test_data_labels=[]\n",
    "\n",
    "for test_sentence, test_label in tqdm(zip(test_data['summarized_text'], test_data['label'])):\n",
    "      try:\n",
    "    input_id, attention_mask, token_type_id = bert_tokenizer(test_sentence, MAX_LEN)\n",
    "\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "    test_data_labels.append(test_label)\n",
    "      except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "test_news_input_ids=np.array(input_ids, dtype=int)\n",
    "test_news_attention_masks=np.array(attention_masks, dtype=int)\n",
    "test_news_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "test_news_inputs=(test_news_input_ids, test_news_attention_masks, test_news_token_type_ids)\n",
    "test_data_labels=np.asarray(test_data_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04755f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.evaluate(test_news_inputs, test_data_labels, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
